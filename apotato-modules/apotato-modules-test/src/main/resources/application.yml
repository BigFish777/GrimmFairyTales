server:
  port: 8081
  servlet:
    context-path: /
spring:
  application:
    name: modules-test
  jackson:
    # 常用，全局设置pojo或被@JsonInclude注解的属性的序列化方式
    default-property-inclusion: NON_NULL #不为空的属性才会序列化,具体属性可看JsonInclude.Include
    date-format: yyyy-MM-dd HH:mm:ss
    locale: en_CH
    time-zone: Asia/Shanghai
    serialization:
      write-dates-as-timestamps: false
  datasource:
    type: com.zaxxer.hikari.HikariDataSource
    driver-class-name: org.postgresql.Driver
    username: liantu
    password: liantu123456
    url: jdbc:postgresql://${DB_HOST:10.4.5.23}:${DB_PORT:54320}/${DB_NAME:forest-test}?currentSchema=${DB_SCHEMA:test}&useSSL=false&serverTimezone=Asia/Shanghai&characterEncoding=utf8&stringtype=unspecified
  kafka:
    #kafka集群地址
    bootstrap-servers: apotato.cn:9093
    #kafka自定义扩展
    #创建topic的分区数
    numPartitions: 8
    #创建topic的副本数
    replicationFactor: 1
    #自动创建topic所需要的topic名称集合
    topics: test1,test2
    #初始化消费者配置
    consumer:
      #当kafka中没有初始offset或offset超出范围时将自动重置offset
      # earliest:重置为分区中最小的offset;
      # latest:重置为分区中最新的offset(消费分区中新产生的数据);
      # none:只要有一个分区不存在已提交的offset,就抛出异常;
      auto-offset-reset: latest
      #是否自动提交offset
      enable-auto-commit: true
      #ky序列化
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      properties:
        #默认的消费组ID
        group:
          id: testGroup
        #消费请求超时时间
        request:
          timeout:
            ms: 180000
        #消费会话超时时间(超过这个时间consumer没有发送心跳,就会触发rebalance操作)
        session:
          timeout:
            ms: 120000
      #提交offset延时(接收到消息后多久提交offset)
      auto:
        commit:
          interval:
            ms: 1000
      #批量消费每次最多消费多少条消息（需要配置批量消费开关）
      #max-poll-records: 50
    #初始化生产者配置
    producer:
      #应答级别:多少个分区副本备份完成时向生产者发送ack确认(可选0、1、all/-1)
      acks: 1
      #批量大小
      batch-size: 16384
      #生产端缓冲区大小
      buffer-memory: 33554432
      #重试次数
      retries: 0
      #提交延时
      # 当生产端积累的消息达到batch-size或接收到消息linger.ms后,生产者就会将消息提交给kafka
      # linger.ms为0表示每接收到一条消息就提交给kafka,这时候batch-size其实就没用了
      properties:
        linger:
          ms: 0
      #Kafka提供的序列化和反序列化类
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.apache.kafka.common.serialization.StringSerializer
    listener:
      #消费端监听的topic不存在时，项目启动会报错(关掉)
      missing-topics-fatal: false
      #设置批量消费
      #type: batch
  redis:
    # Redis数据库索引（默认为0）
    database: 0
    # Redis服务器地址
    host: 127.0.0.1
    # Redis服务器连接端口
    port: 63790
    # 密码
    password: 123456
    jedis:
      pool:
        # 连接池最大连接数（使用负值表示没有限制）
        max-active: 8
        # 连接池最大阻塞等待时间（使用负值表示没有限制）
        max-wait: 1
        # 连接池中的最大空闲连接
        max-idle: 8
        # 连接池中的最小空闲连接
        min-idle: 1
    # 连接超时时间（毫秒）
    timeout: 5000
mybatis-plus:
  mapper-locations: classpath*:mapper/*.xml
  #  type-enums-package: org.fatewa.enums
  # 以下配置均有默认值,可以不设置
  global-config:
    db-config:
      #数据库类型
      id-type: auto
      logic-delete-field: is_delete
      logic-delete-value: true
      logic-not-delete-value: false
  configuration:
    # 是否开启自动驼峰命名规则映射:从数据库列名到Java属性驼峰命名的类似映射
    map-underscore-to-camel-case: true
    # 如果查询结果中包含空值的列，则 MyBatis 在映射的时候，不会映射这个字段
    call-setters-on-nulls: true
    # 这个配置会将执行的sql打印出来，在开发或测试的时候可以用
    log-impl: org.apache.ibatis.logging.stdout.StdOutImpl

